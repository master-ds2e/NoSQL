{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB\n",
    "\n",
    "MongoDB is a cross-platform document-oriented database program. Classified as a NoSQL database program. Each database in MongoDB contains collections. A collection is an organized store of documents. A document is a way to organize data as a set of key-value pairs.\n",
    "\n",
    "MongoDB uses JSON-like documents (BSON) with optional schemas. MongoDB is developed by MongoDB Inc. and licensed under the Server Side Public License (SSPL). \n",
    "\n",
    "Why do we do a focus on MongoDB in this course ?\n",
    "\n",
    "https://db-engines.com/en/ranking\n",
    "https://trends.google.com/trends/?geo=US\n",
    "\n",
    "\n",
    "MongoDB is a document-oriented DB, i.e it is designed to retrieve and manage document-oriented information, also known as semi-structured data. \n",
    "\n",
    "MongoDB and the CAP theorem [1](https://stackoverflow.com/questions/11292215/where-does-mongodb-stand-in-the-cap-theorem)\n",
    "\n",
    "\n",
    "Structure:\n",
    "- [MongoDB first steps](#First_steps)\n",
    "- [CRUD operations](#CRUD)\n",
    "- [Speed up query in MongoDB and the use of Index.](#Index)\n",
    "- [Copy/Merge/Dump collections in MongoDB](#Copy)\n",
    "- [MongoDB store specific kind of data](#Specific)\n",
    "- [Bulk operations](#Bulk)\n",
    "- [Timeout issues](#Timeout)\n",
    "- [Security with MongoDB](#Security)\n",
    "- [MongoDB Atlas](#Atlas)\n",
    "- [Exercises](#Exercises)\n",
    "<a name=\"First_steps\"></a>\n",
    "## MongoDB first steps.\n",
    "\n",
    "At this point you should have MongoDB and MongoDB compass installed (check the readme).\n",
    "launch your MongoDB server. Check that everything works. On the cmd line (or terminal) run\n",
    "\n",
    "```\n",
    "pip install pymongo\n",
    "```\n",
    "\n",
    "and open your Python IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# open connection at port 27017 https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "# create db tutorial\n",
    "mydb = client[\"tutorial\"]\n",
    "# create collection example\n",
    "collection = mydb[\"example\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random dict\n",
    "post = {\"authors\" : [\"Auteur1\",\"Auteur2\",\"Auteur3\"],\n",
    "         \"title\" : \"This is paper 1\",\n",
    "         \"affiliations\" : [\"University of Mannheim\",\"University of Strasbourg\",\"University of wonders\"],\n",
    "         \"ref\" : [\"This is ref 1\",\"This is ref 2\",\"This is ref 3\"]}\n",
    "# Inserting this single dict in mongodb\n",
    "collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same operation as before but with another dict\n",
    "\n",
    "post = {\"authors\" : [\"Auteur1\"],\n",
    "         \"title\" : \"This is paper 2\",\n",
    "         \"affiliations\" : [\"University of Turing\"],\n",
    "         \"ref\" : [\"This is ref 1\",\"This is ref 2\"]}\n",
    "\n",
    "collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"CRUD\"></a>\n",
    "## CRUD Operation\n",
    "\n",
    "What we have done right now is called a Create operation. \n",
    "In computer programming, create, read (aka retrieve), update, and delete (CRUD) are the four basic functions of persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE but not with insert_one, insert_many inserts multiple dicts that are contained by a list\n",
    "# insert_many faster that insert_one (avoiding overhead of the commit).\n",
    "\n",
    "import datetime\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"example\"]\n",
    "\n",
    "# new_posts = list of dicts [{},{}]\n",
    "new_posts = [{\"author\": \"Mike\",\n",
    "              \"title\":\"Python is fun\",\n",
    "               \"text\": \"Another post!\",\n",
    "               \"tags\": [\"bulk\", \"insert\"],\n",
    "              # date object format (year,month,day,hour,minute)\n",
    "               \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n",
    "              {\"author\": \"Eliot\",\n",
    "               \"title\": \"MongoDB is fun\",\n",
    "               \"text\": \"and pretty easy too!\",\n",
    "               \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]\n",
    "\n",
    "collection.insert_many(new_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"example\"]\n",
    "\n",
    "# in each it insert the dict {\"x\":1}\n",
    "for i in tqdm.tqdm(range(3)):\n",
    "    post = {\"x\":1}\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ\n",
    "\n",
    "# get list of collections\n",
    "print(mydb.list_collection_names())\n",
    "\n",
    "# get a document\n",
    "doc = collection.find_one()\n",
    "print(doc)\n",
    "\n",
    "# get a specific document\n",
    "\n",
    "doc = collection.find_one({\"title\":\"This is paper 2\"})\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all docs\n",
    "docs = collection.find()\n",
    "print(docs)\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the query returns a generator. In other words you can load heavy DBs since you won't have in RAM the whole thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE : 3 methods  update_one(),update_many(),find_one_and_update()\n",
    "# multiple modifiers: https://docs.mongodb.com/manual/reference/operator/update-field/\n",
    "# upsert = True if no doc find add this one\n",
    "collection.update_one({\"author\": \"Mike\"},\n",
    "                      {'$set': {\n",
    "                          'author': \"not_mike\"\n",
    "                          }\n",
    "                      }, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update every doc where \"x\" = 1, increment the value by 3\n",
    "collection.update_many({'x': 1}, {'$inc': {'x': 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find one document where \"author\" = not_mike and set the \"done\" field as \"Mike\"\n",
    "doc = collection.find_one_and_update({'author': \"not_mike\"}, {'$set': {'done': \"Mike\"}})\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete statement\n",
    "\n",
    "# print number of document with the key x = 4\n",
    "print(\"init count: \", collection.count_documents({'x': 4}))\n",
    "# Delete one\n",
    "collection.delete_one({'x': 4})\n",
    "# print number of document with the key x = 4 -1 document compared to the init query\n",
    "print(\"after delete one: \", collection.count_documents({'x': 4}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all\n",
    "collection.delete_many({'x': 4})\n",
    "print(\"after delete many: \", collection.count_documents({'x': 4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always I did not go through everything. You will learn more during TODOs but you can use the [documentation](https://docs.mongodb.com/manual/crud/) of MongoDB (Warning its meant for the mongodb terminal not pymongo, but it gives you an idea of what can be done, it's up to you to search how to do it on pymongo). Let's do a real example with everything we have seen. We will use arXiv api. We will work more on api next year so don't worry if you don't understand everything (again not in exam) but here's a short intro. The codes for oaipmh and api are also available on [github](https://github.com/Kwirtz/arxiv_api2mongodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# https://info.arxiv.org/help/oa/index.html to get all ids of papers in a set\n",
    "\n",
    "# Function taken from the package sickle\n",
    "def xml_to_dict(tree, paths=None, nsmap=None, strip_ns=False):\n",
    "    \"\"\"Convert an XML tree to a dictionary.\n",
    "    :param tree: etree Element\n",
    "    :type tree: :class:`lxml.etree._Element`\n",
    "    :param paths: An optional list of XPath expressions applied on the XML tree.\n",
    "    :type paths: list[basestring]\n",
    "    :param nsmap: An optional prefix-namespace mapping for conciser spec of paths.\n",
    "    :type nsmap: dict\n",
    "    :param strip_ns: Flag for whether to remove the namespaces from the tags.\n",
    "    :type strip_ns: bool\n",
    "    \"\"\"\n",
    "    # if xpath empty take every path (.//)\n",
    "    paths = paths or ['.//']\n",
    "    nsmap = nsmap or {}\n",
    "    # defaultdict = never return a keyerror but an empty list\n",
    "    fields = defaultdict(list)\n",
    "    for path in paths:\n",
    "        elements = tree.findall(path, nsmap)\n",
    "        for element in elements:\n",
    "            tag = re.sub(\n",
    "                r'\\{.*\\}', '', element.tag) if strip_ns else element.tag\n",
    "            fields[tag].append(element.text)\n",
    "    return dict(fields)\n",
    "\n",
    "# List of sets from arxiv\n",
    "sets = [\"cs\"]\n",
    "# Parser that can be used as argument in different module. remove_blank_text to deal with newline/tab\n",
    "# recover in case of broken xml, resolve_entities = True returns text directly\n",
    "XMLParser = etree.XMLParser(remove_blank_text=True, recover=True, resolve_entities=False)\n",
    "\n",
    "for set_ in tqdm.tqdm(sets):\n",
    "    # requests oaipmh from arxiv with set\n",
    "    response = requests.get(\"http://export.arxiv.org/oai2?verb=ListIdentifiers&set={}&metadataPrefix=oai_dc\".format(set_))\n",
    "    # init the lxml object using the parser\n",
    "    tree = etree.XML(response.content, parser=XMLParser)\n",
    "    # Transform the xml in dict\n",
    "    papers = xml_to_dict(tree=tree)\n",
    "    # get ids of every element in dict. key = {http://www.openarchives.org/OAI/2.0/}identifier\n",
    "    ids = [id_.split(\":\")[-1] for id_ in papers[\"{http://www.openarchives.org/OAI/2.0/}identifier\"]]\n",
    "    # Write ids in txt file using list comprehension\n",
    "    arxiv_txt = open('data/Chap3/arxiv_cs.txt', 'a')\n",
    "    [arxiv_txt.write(id_ + \"\\n\") for id_ in ids]\n",
    "    arxiv_txt.close()\n",
    "    token = papers[\"{http://www.openarchives.org/OAI/2.0/}resumptionToken\"][0]\n",
    "    time.sleep(20)\n",
    "    # for sets where more than one iteration is needed (e.g you find a resumptionToken) do a while loop until no token\n",
    "    done = False\n",
    "    while done == False:\n",
    "            response = requests.get(\"http://export.arxiv.org/oai2?verb=ListIdentifiers&resumptionToken={}\".format(token))\n",
    "            tree = etree.XML(response.content, parser=XMLParser)\n",
    "            papers = xml_to_dict(tree=tree)\n",
    "            ids = [id_.split(\":\")[-1] for id_ in papers[\"{http://www.openarchives.org/OAI/2.0/}identifier\"]]\n",
    "            if len(ids) != 10000:\n",
    "                done = True\n",
    "            arxiv_txt = open('data/Chap3/arxiv_cs.txt', 'a')\n",
    "            [arxiv_txt.write(id_ + \"\\n\") for id_ in ids]\n",
    "            arxiv_txt.close()\n",
    "            token = papers[\"{http://www.openarchives.org/OAI/2.0/}resumptionToken\"][0]\n",
    "            time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import tqdm\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "# For each id get all the metadata https://info.arxiv.org/help/api/basics.html#python_simple_example\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"arxiv_api\"]\n",
    "\n",
    "# get list of ids previously downloaded\n",
    "with open(\"data/Chap3/arxiv_cs.txt\",\"r\") as lines:\n",
    "    ids = list(set(lines.read().split(\"\\n\")[0:-2]))\n",
    "\n",
    "#init list of ids and iteration\n",
    "ids_query = []\n",
    "\n",
    "# loop through ids\n",
    "for id_ in tqdm.tqdm(ids):\n",
    "    #append id to list\n",
    "    ids_query.append(id_)\n",
    "    # if len list = 100 \n",
    "    if len(ids_query) == 100 :\n",
    "        # collapse list of id\n",
    "        ids_query = \",\".join(ids_query)\n",
    "        # query the api for the 100 ids\n",
    "        response = requests.get('http://export.arxiv.org/api/query?id_list={}&max_results=100'.format(ids_query))\n",
    "        # parse response\n",
    "        feed = feedparser.parse(response.content)\n",
    "        # commit the 100 papers found\n",
    "        list_of_insertion = []\n",
    "        for entry in feed.entries:\n",
    "            list_of_insertion.append(dict(entry))\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        ids_query = []\n",
    "        time.sleep(1/3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to do some exercise ! TODOs 1-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Index\"></a>\n",
    "## Speed up query in MongoDB and the use of Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no id is specified mongo creates automatically an ObjectId for each inserted item.\n",
    "The 12-byte ObjectId value consists of:\n",
    "- a 4-byte timestamp value, representing the ObjectId’s creation, measured in seconds since the Unix epoch\n",
    "- a 5-byte random value\n",
    "- a 3-byte incrementing counter, initialized to a random value\n",
    "\n",
    "You can also create your own index. Why would you do that ? Speeds up query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "# Note that if you use insert_one, this operation will take more than an hour\n",
    "# init list of insertion for insert_many\n",
    "\n",
    "list_of_insertion = []\n",
    "for i in tqdm.tqdm(range(10000000)):\n",
    "    post = {\"user_id\":i,\n",
    "           \"user_name\":\"John\"}\n",
    "    list_of_insertion.append(post)\n",
    "    #if iteration divisible by 15k then insert the list and reset the list\n",
    "    if i % 15000 == 0:\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        list_of_insertion = []\n",
    "        \n",
    "collection.insert_many(list_of_insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain()['executionStats'] information about the query, might be usefull if you are interested in why it takes time\n",
    "collection.find( { \"user_id\": 7000000 } ).explain()['executionStats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index syntax = list of tuples, tuple =(\"name of key which will be indexed\",order 1 = ascending)\n",
    "collection.create_index([ (\"user_id\",1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.find( { \"user_id\": 7000000 } ).explain()['executionStats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although creating an index speeds up some operations, don't use it everywhere.\n",
    "Creating an index works well on fields that have unique values. Using index everywhere is detrimental and eats up your RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives you the number of bytes of the index *(10**-9) bytes to GB\n",
    "print(mydb.command('collStats', 'benchmark')[\"totalIndexSize\"]*(10**-9)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does creating an index exactly do ? Creates a new column with ordered index. Indexes improve the speed of search operations in database because instead of searching the whole document, the search is performed on the indexes that holds only few fields.\n",
    "Look up the documentation to learn more about index and some things to speed up queries\n",
    "https://docs.mongodb.com/manual/tutorial/optimize-query-performance-with-indexes-and-projections/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Copy\"></a>\n",
    "## Copy/Merge/Dump collections in MongoDB\n",
    "\n",
    "Imagine you want to put a collection from one DB to another, one possibility is to read from one DB and write to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "\n",
    "# Create a new db that will be dumped in tutorial\n",
    "mydb_old = client[\"tutorial_2\"]\n",
    "collection_old = mydb_old[\"example_to_dump\"]\n",
    "\n",
    "# random information, imagine it's metadata to a blogpost\n",
    "new_posts = [{\"author\": \"Augustin\",\n",
    "              \"title\":\"This is a paper from another DB\",\n",
    "               \"text\": \"Another post!\",\n",
    "               \"affiliation\": \"University of Strasbourg\",\n",
    "               \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n",
    "              {\"author\": \"Cournot\",\n",
    "               \"title\": \"This is also another paper from another DB\",\n",
    "               \"text\": \"and pretty easy too!\",\n",
    "               \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]\n",
    "\n",
    "collection_old.insert_many(new_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"] \n",
    "\n",
    "#just iterate through every docs and insert it in a new collection in the og db.\n",
    "docs = collection_old.find()\n",
    "for doc in docs:\n",
    "    mydb[collection_old.name].insert_one(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could do the same using the mongo shell (see there https://stackoverflow.com/questions/11554762/how-to-copy-a-collection-from-one-database-to-another-in-mongodb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more useful is the merge of collections. We have 2 collections that we want to merge (i.e benchmark, benchmark_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark 2\n",
    "\n",
    "import tqdm\n",
    "\n",
    "collection = mydb[\"benchmark_2\"]\n",
    "\n",
    "# Note that if you use insert_one, this operation will take more than an hour\n",
    "list_of_insertion = []\n",
    "\n",
    "# range of 1 to 100k but increment by 2 instead of 1\n",
    "for i in tqdm.tqdm(range(1,100000,2)):\n",
    "    post = {\"user_id\":i,\n",
    "           \"random_value\":i*100}\n",
    "    list_of_insertion.append(post)\n",
    "    if i % 15000 == 0:\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        list_of_insertion = []\n",
    "collection.insert_many(list_of_insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "# lookup/unwind/project are a type of aggregation pipeline, Documents pass through the stages of a pipeline in sequence.\n",
    "# Initial collection in collection.aggregate. lookup in the collection \"from\", match on localfield (init_collection)\n",
    "# and foreignfield (from_collection). call the key-value of the from_collection as \"cellmodels\" results = \n",
    "#{\"cellmodels\":[{key-value}:{key-value}]}\n",
    "# unwind cellmodels = expand array\n",
    "# project = select fields\n",
    "\n",
    "pipeline = [{'$lookup': \n",
    "                {'from' : 'benchmark_2',\n",
    "                 'localField' : 'user_id',\n",
    "                 'foreignField' : 'user_id',\n",
    "                 'as' : 'cellmodels'}},\n",
    "            {'$unwind': '$cellmodels'},\n",
    "            {'$project': \n",
    "                {'user_id':1,\"user_name\":1, 'cellmodels.user_id':1, 'cellmodels.random_value':1}} \n",
    "             ]\n",
    "\n",
    "documents = collection.aggregate(pipeline)\n",
    "for i in range(20):\n",
    "    print(next(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different [merges](https://www.dofactory.com/sql/join):\n",
    "\n",
    "![sql](./img/sql-joins.png \"sql-joins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lookup is part of a bigger feature of MongoDB called \"Aggregation\". CRUD operations should be enough most of the times especially when used with python. For example for the merge seen before you could do it multiple ways but, to the best of my knowledge, lookup is the most efficient. \n",
    "\n",
    "I recommend you to read more about this Aggregation process but it's outside of the scope of this lecture. (Maybe more exercises at the end of the course ? merge, match and so on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know the basic operations on MongoDB. To read more on queries and other stuff: https://docs.mongodb.com/manual/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "docs = collection.aggregate([{ \"$sample\": { \"size\": 10000 } }])\n",
    "\n",
    "print(len(list(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO time 8-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Specific\"></a>\n",
    "## MongoDB store specific kind of data.\n",
    "\n",
    "We will finish the Chapter II by discussing a neat trick to store different data type in MongoDB.\n",
    "Imagine you are working on pictures, more specifically you try to scrap website and sometimes you get images you want to store. You'd like to use MongoDB to centralize the data in one DB. How would one do it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the PIL library to load the image but you can also use other libraries like opencv\n",
    "from PIL import Image\n",
    "# Using matplotlib to show the image\n",
    "from matplotlib import pyplot\n",
    "image = Image.open('data/Chap3/FSEG.jpg')\n",
    "pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the image is loaded we need to convert it to a numpy array\n",
    "import numpy as np\n",
    "from bson.binary import Binary\n",
    "import pickle\n",
    "import pymongo\n",
    "\n",
    "# img to np array\n",
    "data = np.asarray(image)\n",
    "# Check the shape\n",
    "print(data.shape)\n",
    "#init dict\n",
    "post = {}\n",
    "# pickle dump the numpy array and then binarize it (reminder MongoDB = Bson = Bytes json)\n",
    "post['image'] = Binary( pickle.dumps( data, protocol=2) ) \n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"image\"]\n",
    "collection.insert_one(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it back\n",
    "import pymongo\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"image\"]\n",
    "docs = collection.find()\n",
    "# load the binary object\n",
    "doc =  pickle.loads(next(docs)[\"image\"])\n",
    "print(doc.shape,type(doc))\n",
    "# From numpy to img\n",
    "image = Image.fromarray(doc)\n",
    "# Show image\n",
    "pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Bulk\"></a>\n",
    "## Bulk operations\n",
    "\n",
    "Updating/Writing in a for loop is fine (one request per write). However if you need to optimize your process you can do multiple operations in a single request. This is called bulk write/update operations. There's two types of bulk operation \"ordered\" and \"unordered\". Unordered can be parallelized hence faster than ordered. let's continue to build on the benchmark collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import names\n",
    "import pymongo\n",
    "\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "docs = collection.find({}, no_cursor_timeout=True).limit(100000)\n",
    "    \n",
    "list_of_insertion = []\n",
    "for doc in tqdm.tqdm(docs):\n",
    "    list_of_insertion.append(pymongo.UpdateOne({\"user_id\": doc[\"user_id\"]},\n",
    "                                               {'$set': {\"user_name\": names.get_full_name()}},\n",
    "                                               upsert = True))\n",
    "\n",
    "collection.bulk_write(list_of_insertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Timeout\"></a>\n",
    "## Timeout issues.\n",
    "\n",
    "MongoDB cursor can timeout at some point, this is because it either took too long to respond to the query or your operation in betweens calls take too much time. the default cursor timeout for mongodb is 10 minutes, you can use the argument \"no_cursor_timeout=True\" which extends the timeout limit to 30 minutes. The other option is to create a session and refresh it automatically. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import names\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "session = client.start_session()\n",
    "\n",
    "docs = collection.find({}, no_cursor_timeout = True, session = session).limit(100000)\n",
    "    \n",
    "list_of_insertion = []\n",
    "for doc in tqdm.tqdm(docs):\n",
    "    list_of_insertion.append(pymongo.UpdateOne({\"user_id\": doc[\"user_id\"]},\n",
    "                                               {'$set': {\"user_name\": names.get_full_name()}},\n",
    "                                               upsert = True))\n",
    "    if len(list_of_insertion) % 1000 == 0:\n",
    "        client.admin.command('refreshSessions', [session.session_id], session=session)\n",
    "\n",
    "collection_output.bulk_write(list_of_insertion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Security\"></a>\n",
    "## Security with MongoDB.\n",
    "For the moment we only worked on MongoDB on localhost, but in a company you'll probably connect to a distant MongoDB.\n",
    "If you want to have a server accesible from distance you need to understand some security protocol.\n",
    "\n",
    "- Bindip: The IP address that mongos or mongod binds to in order to listen for connections from applications. You may attach mongos or mongod to any interface. When attaching mongos or mongod to a publicly accessible interface, ensure that you have implemented proper authentication and firewall restrictions to protect the integrity of your database. 127.0.0.1 is the default bindip = localhost. 0.0.0.0 every ip adress on your network becomes the hostname.\n",
    "\n",
    "- You need a security protocol to avoid random people connecting to your DB: Setup an authenticate protocol and disallow entrant connection in your firewall (except a whitelist of ip adress you'll use to connect to the host).\n",
    "\n",
    "To create an authentification protocol start by creating an admin user. Start up your server and open a mongo shell (just type mongo in a terminal)\n",
    "\n",
    "```\n",
    "mongo\n",
    "use admin\n",
    "\n",
    "db.createUser(\n",
    "      {\n",
    "          user: \"tutorial_2\",\n",
    "          pwd: passwordPrompt(),\n",
    "          roles: [ \"root\" ]\n",
    "      }\n",
    "  )\n",
    "```\n",
    "\n",
    "Now restart your MongoDB server but with the argument --auth in the cmd line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New line if you run with an auth\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://tutorial:tutorial@localhost:27017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in connecting to your server from an other ip address please read the following [documentation](https://docs.mongodb.com/manual/tutorial/configure-windows-netsh-firewall/). But be wary, you are entering a new world. If you open your company connection and something goes wrong, you will be held responsible and could get fired. Only use it if you are sure you understand what is going on. (Using mongodb in the university and opening its connection to everyone might results to you being banned from the wifi access for example because it causes a security breach) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chances are if you go into a company you won't have to setup all this, all you will have to do is use the username, password and hostip they give you. Most of the time companies uses something called MongoDB Atlas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Atlas\"></a>\n",
    "## MongoDB Atlas\n",
    "\n",
    "Why MongoDB Atlas ?\n",
    "\n",
    "- Automated Security Features, you don't have to bother with Ip stuff, MongoDB Atlas do it for you.\n",
    "- Built-In Replication. If one of their server is down you can still access your DB.\n",
    "- Backups and Point-In-Time Recovery - helps you go back in time (sort of like a versioning type of thing), if you mess up (corrupt your DB) than you can use a previous version of your DB.\n",
    "- Fine-Grained Monitoring — Tons of info/graph.\n",
    "- Automated Patching and One-Click Upgrades\n",
    "\n",
    "Read more on https://medium.com/@nparsons08/mongodb-atlas-technical-overview-benefits-9e4cff27a75e\n",
    "\n",
    "MongoDB Atlas has a free small sandbox cluster designed to test and train yourself on clusters.\n",
    "\n",
    "Let's get started !\n",
    "\n",
    "https://www.mongodb.com/cloud/atlas/lp/try2?utm_source=google&utm_campaign=gs_emea_france_search_brand_atlas_desktop&utm_term=mongodb%20atlas&utm_medium=cpc_paid_search&utm_ad=e&utm_ad_campaign_id=1718986507&gclid=Cj0KCQjwoJX8BRCZARIsAEWBFMLxFgZxasM-cyNmMhJjE7pPUpbEObrCW1RPC8lcrNfuTRFVMpXRAGcaAmWxEALw_wcB\n",
    "\n",
    "If you have any trouble or missed a part during lecture please read more on:\n",
    "https://university.mongodb.com/mercury/M001/2020_October_6/chapter/Chapter_1_What_is_MongoDB_/lesson/5f32deb504e9ffc01ac9586c/problem\n",
    "\n",
    "Now let's interact with MongoDB Atlas using pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install the next package to use URI from mongo atlas.\n",
    "!python -m pip install pymongo[srv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# pymongo.MongoClient can be found on mongodbatlas, you can check the image folder if lost\n",
    "client = pymongo.MongoClient('mongodb+srv://test:tutorial.@example.ytktu.mongodb.net/sample_airbnb?retryWrites=true&w=majority')\n",
    "mydb = client[\"sample_airbnb\"]\n",
    "collection = mydb[\"listingsAndReviews\"]\n",
    "\n",
    "docs = collection.find()\n",
    "next(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every object that can be transformed to a numpy array(tensor) can be stocked in a MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"control\"></a>\n",
    "## Control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "docs = collection.find({\"user_id\":{\"$exists\":1,\"$gt\":10000}}).limit(2)\n",
    "list(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Exercises\"></a>\n",
    "### Exercises\n",
    "\n",
    "#### CRUD operations\n",
    "\n",
    "Create a new db name Todo and a new collection named \"CRUD_exercise\" and do the following:\n",
    "\n",
    "**TODO 1**: Take the dict created in the TODO 4 in chapter I and save it in the collection \"CRUD_exercise\".\n",
    "\n",
    "**TODO 2**: Insert 3 documents with key = x and values = 1, delete one of them. Which one is deleted first ? the most recent or oldest one ? increment the value of x to 4.\n",
    "\n",
    "**TODO 3**: Insert the dict created in the TODO 6 Chapter I in the example collection.\n",
    "\n",
    "**TODO 4**: Get documents where authors key exist in the collection \"CRUD_exercise\".\n",
    "\n",
    "**TODO 5**: Change the documents where x = 4 to x = 1.\n",
    "\n",
    "**TODO 6**: Find documents where author is not_mike and set author as real_mike.\n",
    "\n",
    "**TODO 7**: Delete documents where author is real_mike.\n",
    "\n",
    "#### Managing DB\n",
    "\n",
    "**TODO 8**: create a collection named \"CRUD_exercise_benchmark\" with 500k observations, ids increment of 2 (sequence:0,2,4,6,...1M). Give a random np.array with a key named \"values\" and use the insert_many. Then create an index on the id and benchmark queries before and after indexing. Did the index help ?\n",
    "\n",
    "**TODO 9**: create a random collection in a random db and put the new collection in the tutorial DB\n",
    "\n",
    "**TODO 10**: What is the difference between an inner join and an outer join ? Is the query seen during course an inner or outer join ? Play with the query to show all the joins.\n",
    "\n",
    "#### Real world problems\n",
    "\n",
    "**TODO 11**:  Use the oaipmh and api code get papers after January 2020 and for \"cs,math,econ\" categories. Insert them in MongoDB. Import only the first 200. How is it sorted ? How can you define your own sort()? Query papers to get papers after 2021, which have 3 authors and with domain \"cs\".\n",
    "\n",
    "**TODO 12**: Do the same as TODO 8 but with the connection to the cluster. Then check the metrics and take screenshot of opcounters, logical size and connections.\n",
    "\n",
    "**TODO 13**: Download a random image and store it in a collection.\n",
    "\n",
    "**TODO 14**: Try to store a pandas dataframe in mongoDB (array with rownames, array with colnames and matrix with values)\n",
    "\n",
    "**TODO 15**: Insert the movie_review.tsv data into mongodb. Then query it to find the number of review that are positive and negative review. Fetch the docs which have \"unexpected\" in their review, how many are they ? Think of a clever way to count the number of words in the review using MongoDB (hint: Transform the review text before the insert in MongoDB) and create a density of number of words per review.\n",
    "\n",
    "**TODO 16**: Download a [sound sample](https://freesound.org/browse/). Try to store it in MongoDB \n",
    "\n",
    "**TODO 17**: Create a collection with 30M observation with a single key : \"year\" which is a random value between 2000-2020. Get documents with year = 2000. Does using an index helps ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After a short introduction on why we are going to focus on MongoDB (Most popular type and DB in the NoSQL domain) we have seen how to work on MongoDB in local, how to perform CRUD operations but also speeding up queries, using complex queries and how to merge multiple DBs/collections. After a short discussion around security on MongoDB, we then move to the cloud version of MongoDB: MongoDB Atlas which facilitates some features (especially security wise). Finishing this chapter is a way to store everything that can be converted into a numpy array.\n",
    "\n",
    "In the next chapter we will see briefly introduce and discuss other DBs that are rank 1 for their type of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1\n",
    "\n",
    "pubmed_cleaned.zip is a file containing a metadatas sample from pubmed articles. Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:\n",
    "\n",
    "1) Create an index, explain your choice of key.\n",
    "\n",
    "2) Delete every paper that was published prior 2019\n",
    "\n",
    "3) How many paper have a single author ? Two authors ?\n",
    "\n",
    "4) What's the last paper inserted in the db ?\n",
    "\n",
    "5) Find articles with null meshwords.\n",
    "\n",
    "6) Choose a keyword you are interested in (machine learning, computer vision,...). Find the number of articles with the choosen keyword in their meshwords, abstract or title.\n",
    "\n",
    "7) What's the number of articles that have atleast one affiliation AND meshwords.\n",
    "\n",
    "8) How many articles have a publishing date after 2020 ?\n",
    "\n",
    "9) Find articles where there's atleast one affiliation from a choosen country (you decide which one).\n",
    "\n",
    "10) Check for any duplicates. (hint: look at the doi or the pmid)\n",
    "\n",
    "11) Remove every articles where the abstract starts with an \"R\".\n",
    "\n",
    "12) Return the list of papers (pmid) where there's atleast one affiliation per author\n",
    "\n",
    "13) Create 500 random samples of the dataset, compute a statistics that you are interested in and check how it behaves through the different samples\n",
    "\n",
    "14) Sandbox exercise: think of a problematic and try to answer it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2\n",
    "\n",
    "authors.zip is a file containing a sample of authors that wrote a paper published on pubmed. Each doc as, at most, 5 keys. \"AND_ID\" is the disambiguated author id. \"pmid_list\" is the list of ids that the author published. \"more_info\" is a list of dict with each dict representing info for a given paper. \"oa04_affiliations\" is a list of dict with each dict representing affiliation info for a given paper. \"oa06_researcher_education\" is a list of dict with each dict containing information on the education of the researcher.\n",
    "\n",
    "Your goal is to **convert the json to a mongo DB** and answer the following questions **USING** mongodb querys:\n",
    "\n",
    "1) Create an index, explain your choice of key.\n",
    "\n",
    "2) What is the average length of \"pmid_list\"\n",
    "\n",
    "3) How many distinct affiliations are there ?\n",
    "\n",
    "4) Find authors with atleast one \"COM\" AffiliationType\n",
    "\n",
    "5) How many authors switched the AffiliationType ?\n",
    "\n",
    "6) Find affiliation with the word \"China\" \n",
    "\n",
    "7) Get the pmids of papers published in 2019\n",
    "\n",
    "8) Count the number of doc with \"oa06_researcher_education\" OR \"oa04_affiliations\" key and with the \"oa06_researcher_education\" AND \"oa04_affiliations\" .\n",
    "\n",
    "9) What's the average \"BeginYear\" of \"oa06_researcher_education\".\n",
    "\n",
    "10) Count the distinct country of \"oa06_researcher_education\"\n",
    "\n",
    "11) Does the length of pmid_list and more_info always match ?\n",
    "\n",
    "12) Does the length of pmid_list and \"oa04_affiliations\" always match ?\n",
    "\n",
    "13) Sandbox exercise: think of a problematic and try to answer it."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
